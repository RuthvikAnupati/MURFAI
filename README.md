# ğŸ™ AI Voice Assistant â€“ FastAPI + Murf AI + AssemblyAI + Google Gemini

![Python](https://img.shields.io/badge/Python-3.10-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-ğŸš€-green)
![MurfAI](https://img.shields.io/badge/Murf%20AI-TTS-orange)
![AssemblyAI](https://img.shields.io/badge/AssemblyAI-STT-yellow)
![GoogleGemini](https://img.shields.io/badge/Google-Gemini-lightblue)
![License](https://img.shields.io/badge/License-MIT-purple)

---

## ğŸ“‚ Project Structure
```
project-root/
â”‚â”€â”€ main.py                  # FastAPI entry point
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ .env                     # Environment variables
â”‚â”€â”€ static/                  # Static assets (audio, video, icons, CSS, JS)
â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”œâ”€â”€ error.wav
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ welcome.html
â”‚   â”œâ”€â”€ v1.mp4
â”‚   â”œâ”€â”€ v2.mp4
â”‚   â””â”€â”€ transcribe/          # Additional static resources
â”‚
â”‚â”€â”€ uploads/                 # Temporary audio uploads
â”‚â”€â”€ templates/               # Jinja2 HTML templates (if any)
â”‚â”€â”€ venv/                    # Virtual environment
â”‚â”€â”€ README.md                # Documentation
```

---

## ğŸ— System Architecture
1. **Frontend / API Client** sends text or file to the backend API.
2. **FastAPI server** processes the request.
3. **Murf AI API** generates the speech file.
4. The generated audio is stored locally or returned as a response.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤ Frontend UI  â”‚â”€â”€â”€â”€â–¶â”‚ ğŸš€ FastAPI API  â”‚â”€â”€â”€â”€â–¶â”‚  ğŸ§  AI Services â”‚
â”‚                  â”‚      â”‚                  â”‚     â”‚                  â”‚
â”‚ â€¢ Audio Record   â”‚      â”‚ â€¢ Pipeline Mgmt  â”‚     â”‚ â€¢ AssemblyAI STT â”‚
â”‚ â€¢ Visualization  â”‚      â”‚ â€¢ Session Memory â”‚     â”‚ â€¢ Google Gemini  â”‚
â”‚ â€¢ Playback       â”‚      â”‚ â€¢ Error Handling â”‚     â”‚ â€¢ Murf AI TTS    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Pipeline Flow
1. **Audio Capture** â†’ Browser records user speech via MediaRecorder API.
2. **Speech Recognition (STT)** â†’ AssemblyAI transcribes audio to text.
3. **AI Processing (LLM)** â†’ Google Gemini (or other LLM) generates a response with personality.
4. **Voice Synthesis (TTS)** â†’ Murf AI converts response text to natural-sounding speech.
5. **Playback** â†’ Browser plays generated voice with real-time waveform visualizations.

---

## âš™ Environment Variables
Create a `.env` file in the project root with:
```
MURF_API_KEY=your_murf_api_key
ASSEMBLYAI_API_KEY=your_assemblyai_key
GOOGLE_API_KEY=your_google_api_key
```

---

## ğŸš€ Installation & Run
```bash
# Clone the repository
git clone https://github.com/yourusername/ai-voice-assistant.git
cd ai-voice-assistant

# Create virtual environment
python -m venv venv
source venv/bin/activate  # For Linux/Mac
venv\Scripts\activate   # For Windows

# Install dependencies
pip install -r requirements.txt

# Run FastAPI server
uvicorn main:app --reload --port 8000
```

---

## ğŸ“œ License
This project is licensed under the MIT License.
